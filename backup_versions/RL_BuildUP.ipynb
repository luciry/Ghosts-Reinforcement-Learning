{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants for cell states\n",
    "EMPTY = 0\n",
    "GOOD_GHOST_PLAYER = 1\n",
    "BAD_GHOST_PLAYER = 2\n",
    "GOOD_GHOST_OPPONENT = 3\n",
    "BAD_GHOST_OPPONENT = 4\n",
    "OPPONENT_CORNER = 5\n",
    "PLAYER_CORNER = 6\n",
    "\n",
    "# Reward structure\n",
    "REACH_CORNER_REWARD = 100\n",
    "CAPTURE_GOOD_GHOST_PENALTY = -50\n",
    "CAPTURE_BAD_GHOST_REWARD = 50\n",
    "OPPONENT_CAPUTRE_YOUR_GOOD_GHOST_PENALTY = -50\n",
    "OPPONENT_CAPUTRE_YOUR_BAD_GHOST_REWARD = 50\n",
    "OPPONENT_CORNER_REWARD = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GhostsEnv:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((6, 6), dtype=int)\n",
    "        self.ghost_positions = []\n",
    "        self.current_phase = 'placement'\n",
    "        self.current_turn = 1\n",
    "        self.placement_count = 0\n",
    "        self.previous_winning_positions = []\n",
    "      \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((6, 6), dtype=int)\n",
    "        self.ghost_positions = []\n",
    "        self.current_phase = 'placement'\n",
    "        self.current_turn = 1\n",
    "        self.placement_count = 0\n",
    "        # Set corner states\n",
    "        self.board[0, 0] = self.board[0, 5] = OPPONENT_CORNER\n",
    "        self.board[5, 0] = self.board[5, 5] = PLAYER_CORNER\n",
    "        return self.board\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = []\n",
    "        if self.current_phase == 'placement':\n",
    "            if self.current_turn == 1:  # Player's turn\n",
    "                rows = [5, 4]\n",
    "            else:  # Opponent's turn\n",
    "                rows = [0, 1]\n",
    "            \n",
    "            for row in rows:\n",
    "                for col in range(1, 5):\n",
    "                    if self.board[row, col] == EMPTY:\n",
    "                        valid_actions.append(4 * (row % 4) + (col - 1))\n",
    "        else:  # Movement phase\n",
    "            for i, (x, y) in enumerate(self.ghost_positions):\n",
    "                if (self.current_turn == 1 and self.board[x, y] in [GOOD_GHOST, BAD_GHOST]) or \\\n",
    "                   (self.current_turn == 2 and self.board[x, y] in [GOOD_GHOST_OPPONENT, BAD_GHOST_OPPONENT]):\n",
    "                    for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                        new_x, new_y = x + dx, y + dy\n",
    "                        if 0 <= new_x < 6 and 0 <= new_y < 6 and self.board[new_x, new_y] == EMPTY:\n",
    "                            valid_actions.append(i * 5 + [(-1, 0), (1, 0), (0, -1), (0, 1)].index((dx, dy)))\n",
    "                    # Add capture action\n",
    "                    for adj_x, adj_y in self.get_adjacent_positions((x, y)):\n",
    "                        if (self.current_turn == 1 and self.board[adj_x, adj_y] in [GOOD_GHOST_OPPONENT, BAD_GHOST_OPPONENT]) or \\\n",
    "                           (self.current_turn == 2 and self.board[adj_x, adj_y] in [GOOD_GHOST, BAD_GHOST]):\n",
    "                            valid_actions.append(i * 5 + 4)\n",
    "                            break\n",
    "        \n",
    "        if not valid_actions:\n",
    "            print(\"Warning: No valid actions available.\")\n",
    "            print(\"Current board state:\")\n",
    "            print(self.board)\n",
    "            print(f\"Current phase: {self.current_phase}\")\n",
    "            print(f\"Current turn: {self.current_turn}\")\n",
    "            print(f\"Placement count: {self.placement_count}\")\n",
    "            print(f\"Ghost positions: {self.ghost_positions}\")\n",
    "        \n",
    "        return valid_actions\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_phase == 'placement':\n",
    "            return self.placement_step(action)\n",
    "        else:\n",
    "            return self.movement_step(action)\n",
    "\n",
    "    def placement_step(self, action):\n",
    "        reward = 0\n",
    "        row = 5 if action < 4 else 4 if self.current_turn == 1 else 0 if action < 4 else 1\n",
    "        col = (action % 4) + 1\n",
    "        \n",
    "        if self.current_turn == 1:  # Player's turn\n",
    "            ghost_type = GOOD_GHOST if self.placement_count < 4 else BAD_GHOST\n",
    "        else:  # Opponent's turn\n",
    "            ghost_type = GOOD_GHOST_OPPONENT if random.random() < 0.5 else BAD_GHOST_OPPONENT\n",
    "\n",
    "        self.board[row, col] = ghost_type\n",
    "        self.ghost_positions.append((row, col))\n",
    "        reward = 10 if ghost_type in [GOOD_GHOST, GOOD_GHOST_OPPONENT] else 5\n",
    "        self.placement_count += 1\n",
    "        self.current_turn = 3 - self.current_turn\n",
    "\n",
    "        if self.placement_count == 16:\n",
    "            self.current_phase = 'movement'\n",
    "            reward += self.reward_for_initial_position()\n",
    "\n",
    "        return self.board, reward, False, {}\n",
    "\n",
    "    def movement_step(self, action):\n",
    "        ghost_index = action // 5\n",
    "        direction = action % 5\n",
    "\n",
    "        if direction == 4:  # Capture action\n",
    "            return self.capture_ghost(ghost_index)\n",
    "        else:\n",
    "            return self.move_ghost(ghost_index, direction)\n",
    "\n",
    "    def move_ghost(self, ghost_index, direction):\n",
    "        current_pos = self.ghost_positions[ghost_index]\n",
    "        new_pos = self.get_new_position(current_pos, direction)\n",
    "\n",
    "        ghost_type = self.board[current_pos]\n",
    "        self.board[current_pos] = EMPTY\n",
    "        self.board[new_pos] = ghost_type\n",
    "        self.ghost_positions[ghost_index] = new_pos\n",
    "\n",
    "        reward, done = self.check_win_conditions()\n",
    "        reward += MOVE_PENALTY\n",
    "\n",
    "        self.current_turn = 3 - self.current_turn\n",
    "        return self.board, reward, done, {}\n",
    "\n",
    "    def capture_ghost(self, ghost_index):\n",
    "        current_pos = self.ghost_positions[ghost_index]\n",
    "        adjacent_positions = self.get_adjacent_positions(current_pos)\n",
    "\n",
    "        for pos in adjacent_positions:\n",
    "            if (self.current_turn == 1 and self.board[pos] in [GOOD_GHOST_OPPONENT, BAD_GHOST_OPPONENT]) or \\\n",
    "               (self.current_turn == 2 and self.board[pos] in [GOOD_GHOST, BAD_GHOST]):\n",
    "                captured_ghost = self.board[pos]\n",
    "                if captured_ghost in [BAD_GHOST_OPPONENT, BAD_GHOST]:\n",
    "                    reward = CAPTURE_BAD_GHOST_REWARD\n",
    "                else:\n",
    "                    reward = CAPTURE_GOOD_GHOST_PENALTY\n",
    "                self.board[pos] = EMPTY\n",
    "                self.ghost_positions.remove(pos)\n",
    "                done = self.check_win_conditions()[1]\n",
    "                self.current_turn = 3 - self.current_turn\n",
    "                return self.board, reward, done, {}\n",
    "\n",
    "        return self.board, MOVE_PENALTY, False, {}\n",
    "\n",
    "    def get_new_position(self, current_pos, direction):\n",
    "        x, y = current_pos\n",
    "        if direction == 0:  # Up\n",
    "            return (x - 1, y)\n",
    "        elif direction == 1:  # Down\n",
    "            return (x + 1, y)\n",
    "        elif direction == 2:  # Left\n",
    "            return (x, y - 1)\n",
    "        elif direction == 3:  # Right\n",
    "            return (x, y + 1)\n",
    "\n",
    "    def get_adjacent_positions(self, pos):\n",
    "        x, y = pos\n",
    "        return [(x + dx, y + dy) for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)] if 0 <= x + dx < 6 and 0 <= y + dy < 6]\n",
    "\n",
    "    def check_win_conditions(self):\n",
    "        if self.player_reaches_corner():\n",
    "            self.store_winning_position()\n",
    "            return REACH_CORNER_REWARD, True\n",
    "        elif self.all_good_ghosts_captured():\n",
    "            return CAPTURE_GOOD_GHOST_PENALTY, True\n",
    "        return 0, False\n",
    "\n",
    "    def player_reaches_corner(self):\n",
    "        return self.board[0, 0] == GOOD_GHOST or self.board[0, 5] == GOOD_GHOST\n",
    "\n",
    "    def all_good_ghosts_captured(self):\n",
    "        return np.sum(self.board == GOOD_GHOST) == 0\n",
    "\n",
    "    def reward_for_initial_position(self):\n",
    "        if not self.previous_winning_positions:\n",
    "            return 0\n",
    "        similarity_score = sum(1 for current, previous in zip(self.ghost_positions, self.previous_winning_positions) if current == previous)\n",
    "        return similarity_score * 2\n",
    "\n",
    "    def store_winning_position(self):\n",
    "        self.previous_winning_positions = self.ghost_positions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class GhostsAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(6*6, 13).to(self.device)  # 6x6 board flattened, 13 possible actions\n",
    "        self.target_model = DQN(6*6, 13).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.update_target_every = 100\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state.flatten(), action, reward, next_state.flatten(), done))\n",
    "\n",
    "    def act(self, state):\n",
    "        valid_actions = self.env.get_valid_actions()\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        state = torch.FloatTensor(state.flatten()).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state)\n",
    "        valid_q_values = q_values[0, valid_actions]\n",
    "        return valid_actions[torch.argmax(valid_q_values).item()]\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, episodes):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        cmap = ListedColormap(['white', 'green', 'red', 'blue', 'yellow', 'purple', 'orange'])\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                # Visualize the game board\n",
    "                ax1.clear()\n",
    "                ax1.imshow(state, cmap=cmap, vmin=0, vmax=6)\n",
    "                ax1.set_title(f\"Episode {episode}, Step {steps}\")\n",
    "                for i in range(6):\n",
    "                    for j in range(6):\n",
    "                        ax1.text(j, i, str(int(state[i, j])), ha='center', va='center')\n",
    "\n",
    "                # Plot rewards\n",
    "                ax2.clear()\n",
    "                ax2.plot(self.episode_rewards)\n",
    "                ax2.set_title(\"Rewards per Episode\")\n",
    "                ax2.set_xlabel(\"Episode\")\n",
    "                ax2.set_ylabel(\"Total Reward\")\n",
    "\n",
    "                plt.draw()\n",
    "                plt.pause(0.1)\n",
    "\n",
    "                # Print detailed information\n",
    "                print(f\"Episode: {episode}, Step: {steps}\")\n",
    "                print(f\"Action: {action}\")\n",
    "                print(f\"Reward: {reward}\")\n",
    "                print(\"Current State:\")\n",
    "                print(state)\n",
    "                print(\"Next State:\")\n",
    "                print(next_state)\n",
    "                print(f\"Done: {done}\")\n",
    "                print(\"-------------------\")\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    self.replay()\n",
    "\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            if episode % self.update_target_every == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            print(f\"Episode {episode} finished. Total Reward: {total_reward}, Steps: {steps}, Epsilon: {self.epsilon:.4f}\")\n",
    "            time.sleep(1)  # Pause between episodes\n",
    "\n",
    "        plt.close()\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def visualize_training(self):\n",
    "        episodes = len(self.episode_rewards)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(episodes), self.episode_rewards)\n",
    "        plt.title('Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.show()\n",
    "\n",
    "        avg_reward = sum(self.episode_rewards) / episodes\n",
    "        max_reward = max(self.episode_rewards)\n",
    "        min_reward = min(self.episode_rewards)\n",
    "\n",
    "        print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "        print(f\"Max Reward: {max_reward}\")\n",
    "        print(f\"Min Reward: {min_reward}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(self.episode_rewards, bins=50)\n",
    "        plt.title('Distribution of Rewards')\n",
    "        plt.xlabel('Reward')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "        window_size = 100\n",
    "        moving_avg = np.convolve(self.episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(len(moving_avg)), moving_avg)\n",
    "        plt.title(f'Moving Average of Rewards (Window Size: {window_size})')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self, episodes):\n",
    "        total_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                self.env.render_board()\n",
    "                print(f\"Action: {action}, Reward: {reward}\")\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1} finished with total reward: {episode_reward}\")\n",
    "\n",
    "        avg_reward = sum(total_rewards) / episodes\n",
    "        print(f\"Average Reward over {episodes} episodes: {avg_reward:.2f}\")\n",
    "        return avg_reward\n",
    "\n",
    "    def play_games(self, num_games=1, human_player=False):\n",
    "        total_rewards = []\n",
    "        for i in range(num_games):\n",
    "            print(f\"\\nGame {i+1}\")\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                self.env.render_board()\n",
    "                \n",
    "                if human_player and self.env.current_turn == 1:\n",
    "                    action = self.env.get_human_action()\n",
    "                else:\n",
    "                    action = self.act(state)\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                print(f\"Action: {action}, Reward: {reward}\")\n",
    "                \n",
    "                state = next_state\n",
    "                self.env.current_turn = 3 - self.env.current_turn  # Switch turns (1 -> 2, 2 -> 1)\n",
    "            \n",
    "            self.env.render_board()\n",
    "            print(f\"Game Over! Total Reward: {total_reward}\")\n",
    "            total_rewards.append(total_reward)\n",
    "        \n",
    "        avg_reward = sum(total_rewards) / num_games\n",
    "        print(f\"\\nAverage Reward over {num_games} games: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GOOD_GHOST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Visualize the training results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m agent\u001b[38;5;241m.\u001b[39mvisualize_training()\n",
      "Cell \u001b[1;32mIn[5], line 75\u001b[0m, in \u001b[0;36mGhostsAgent.train\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     74\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 75\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Visualize the game board\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36mGhostsEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplacement\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplacement_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovement_step(action)\n",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m, in \u001b[0;36mGhostsEnv.placement_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     68\u001b[0m col \u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Player's turn\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     ghost_type \u001b[38;5;241m=\u001b[39m \u001b[43mGOOD_GHOST\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplacement_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m BAD_GHOST\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Opponent's turn\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     ghost_type \u001b[38;5;241m=\u001b[39m GOOD_GHOST_OPPONENT \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m BAD_GHOST_OPPONENT\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GOOD_GHOST' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMkAAAH/CAYAAABNS4qDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAl/0lEQVR4nO3df2zX9Z3A8VcpttXMVjyO8uPqON05t6ngQLrqiPHSG4mGHX9cxukCHHF6bpxxNHcT/EHn3Cjn1JBMHJHpueTmwWbUWwbBc72RxcmFjB+JO0Hj0MEta4Xb0TLcqLSf+2O37jpA+Ra+/eHr8Ui+f/DZ+9Pvu3uLvvLst99vRVEURQAAAABAYmOGewMAAAAAMNxEMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0is5kv3whz+MuXPnxuTJk6OioiKeffbZd71ny5Yt8dGPfjSqq6vjAx/4QDzxxBOD2CoAAOVkzgMAMis5kh05ciSmTZsWa9asOaX1r7/+elx//fVx7bXXxq5du+Lzn/98fOYzn4nnnnuu5M0CAFA+5jwAILOKoiiKQd9cURHPPPNMzJs376Rr7rjjjti4cWP85Cc/6b/213/913Ho0KHYvHnzYJ8aAIAyMucBANmMLfcTbN26NZqbmwdcmzNnTnz+858/6T1Hjx6No0eP9v+5r68vfvnLX8Yf/dEfRUVFRbm2CgC8hxRFEYcPH47JkyfHmDHehrUczHkAwHAo15xX9kjW0dER9fX1A67V19dHd3d3/PrXv46zzz77uHva2tri3nvvLffWAIAE9u/fH3/yJ38y3Nt4TzLnAQDD6UzPeWWPZIOxfPnyaGlp6f9zV1dXXHDBBbF///6ora0dxp0BAKNFd3d3NDQ0xLnnnjvcW+H/MecBAKerXHNe2SPZxIkTo7Ozc8C1zs7OqK2tPeFPFyMiqquro7q6+rjrtbW1hicAoCR+ha98zHkAwHA603Ne2d+go6mpKdrb2wdce/7556OpqancTw0AQBmZ8wCA95KSI9mvfvWr2LVrV+zatSsifvvR37t27Yp9+/ZFxG9fQr9w4cL+9bfeemvs3bs3vvCFL8SePXvikUceiW9/+9uxdOnSM/MdAABwRpjzAIDMSo5kP/7xj+OKK66IK664IiIiWlpa4oorrogVK1ZERMQvfvGL/kEqIuJP//RPY+PGjfH888/HtGnT4sEHH4xvfOMbMWfOnDP0LQAAcCaY8wCAzCqKoiiGexPvpru7O+rq6qKrq8t7VQAAp8T8MDo4JwCgVOWaH8r+nmQAAAAAMNKJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpDSqSrVmzJqZOnRo1NTXR2NgY27Zte8f1q1evjg9+8INx9tlnR0NDQyxdujR+85vfDGrDAACUjzkPAMiq5Ei2YcOGaGlpidbW1tixY0dMmzYt5syZE2+++eYJ1z/55JOxbNmyaG1tjd27d8djjz0WGzZsiDvvvPO0Nw8AwJljzgMAMis5kj300ENx8803x+LFi+PDH/5wrF27Ns4555x4/PHHT7j+xRdfjKuvvjpuvPHGmDp1anziE5+IG2644V1/KgkAwNAy5wEAmZUUyXp6emL79u3R3Nz8+y8wZkw0NzfH1q1bT3jPVVddFdu3b+8flvbu3RubNm2K66677jS2DQDAmWTOAwCyG1vK4oMHD0Zvb2/U19cPuF5fXx979uw54T033nhjHDx4MD7+8Y9HURRx7NixuPXWW9/xZfhHjx6No0eP9v+5u7u7lG0CAFAicx4AkF3ZP91yy5YtsXLlynjkkUdix44d8fTTT8fGjRvjvvvuO+k9bW1tUVdX1/9oaGgo9zYBACiROQ8AeC+pKIqiONXFPT09cc4558RTTz0V8+bN67++aNGiOHToUPzrv/7rcffMnj07Pvaxj8VXv/rV/mv//M//HLfcckv86le/ijFjju90J/oJY0NDQ3R1dUVtbe2pbhcASKy7uzvq6urMD6fInAcAjBblmvNKeiVZVVVVzJgxI9rb2/uv9fX1RXt7ezQ1NZ3wnrfeeuu4AamysjIiIk7W56qrq6O2tnbAAwCA8jHnAQDZlfSeZBERLS0tsWjRopg5c2bMmjUrVq9eHUeOHInFixdHRMTChQtjypQp0dbWFhERc+fOjYceeiiuuOKKaGxsjNdeey3uueeemDt3bv8QBQDA8DPnAQCZlRzJ5s+fHwcOHIgVK1ZER0dHTJ8+PTZv3tz/Jq/79u0b8BPFu+++OyoqKuLuu++On//85/HHf/zHMXfu3PjKV75y5r4LAABOmzkPAMispPckGy7eUwQAKJX5YXRwTgBAqUbEe5IBAAAAwHuRSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHqDimRr1qyJqVOnRk1NTTQ2Nsa2bdvecf2hQ4diyZIlMWnSpKiuro6LL744Nm3aNKgNAwBQPuY8ACCrsaXesGHDhmhpaYm1a9dGY2NjrF69OubMmROvvPJKTJgw4bj1PT098Rd/8RcxYcKEeOqpp2LKlCnxs5/9LM4777wzsX8AAM4Qcx4AkFlFURRFKTc0NjbGlVdeGQ8//HBERPT19UVDQ0PcdtttsWzZsuPWr127Nr761a/Gnj174qyzzhrUJru7u6Ouri66urqitrZ2UF8DAMjF/FA6cx4AMBqUa34o6dcte3p6Yvv27dHc3Pz7LzBmTDQ3N8fWrVtPeM93v/vdaGpqiiVLlkR9fX1ceumlsXLlyujt7T3p8xw9ejS6u7sHPAAAKB9zHgCQXUmR7ODBg9Hb2xv19fUDrtfX10dHR8cJ79m7d2889dRT0dvbG5s2bYp77rknHnzwwfjyl7980udpa2uLurq6/kdDQ0Mp2wQAoETmPAAgu7J/umVfX19MmDAhHn300ZgxY0bMnz8/7rrrrli7du1J71m+fHl0dXX1P/bv31/ubQIAUCJzHgDwXlLSG/ePHz8+Kisro7Ozc8D1zs7OmDhx4gnvmTRpUpx11llRWVnZf+1DH/pQdHR0RE9PT1RVVR13T3V1dVRXV5eyNQAAToM5DwDIrqRXklVVVcWMGTOivb29/1pfX1+0t7dHU1PTCe+5+uqr47XXXou+vr7+a6+++mpMmjTphIMTAABDz5wHAGRX8q9btrS0xLp16+Kb3/xm7N69Oz772c/GkSNHYvHixRERsXDhwli+fHn/+s9+9rPxy1/+Mm6//fZ49dVXY+PGjbFy5cpYsmTJmfsuAAA4beY8ACCzkn7dMiJi/vz5ceDAgVixYkV0dHTE9OnTY/Pmzf1v8rpv374YM+b37a2hoSGee+65WLp0aVx++eUxZcqUuP322+OOO+44c98FAACnzZwHAGRWURRFMdybeDfd3d1RV1cXXV1dUVtbO9zbAQBGAfPD6OCcAIBSlWt+KPunWwIAAADASCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQ3qEi2Zs2amDp1atTU1ERjY2Ns27btlO5bv359VFRUxLx58wbztAAAlJk5DwDIquRItmHDhmhpaYnW1tbYsWNHTJs2LebMmRNvvvnmO973xhtvxN///d/H7NmzB71ZAADKx5wHAGRWciR76KGH4uabb47FixfHhz/84Vi7dm2cc8458fjjj5/0nt7e3vj0pz8d9957b1x44YWntWEAAMrDnAcAZFZSJOvp6Ynt27dHc3Pz77/AmDHR3NwcW7duPel9X/rSl2LChAlx0003ndLzHD16NLq7uwc8AAAoH3MeAJBdSZHs4MGD0dvbG/X19QOu19fXR0dHxwnveeGFF+Kxxx6LdevWnfLztLW1RV1dXf+joaGhlG0CAFAicx4AkF1ZP93y8OHDsWDBgli3bl2MHz/+lO9bvnx5dHV19T/2799fxl0CAFAqcx4A8F4ztpTF48ePj8rKyujs7BxwvbOzMyZOnHjc+p/+9KfxxhtvxNy5c/uv9fX1/faJx46NV155JS666KLj7quuro7q6upStgYAwGkw5wEA2ZX0SrKqqqqYMWNGtLe391/r6+uL9vb2aGpqOm79JZdcEi+99FLs2rWr//HJT34yrr322ti1a5eX1wMAjBDmPAAgu5JeSRYR0dLSEosWLYqZM2fGrFmzYvXq1XHkyJFYvHhxREQsXLgwpkyZEm1tbVFTUxOXXnrpgPvPO++8iIjjrgMAMLzMeQBAZiVHsvnz58eBAwdixYoV0dHREdOnT4/Nmzf3v8nrvn37YsyYsr7VGQAAZWDOAwAyqyiKohjuTbyb7u7uqKuri66urqitrR3u7QAAo4D5YXRwTgBAqco1P/hRIAAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACkJ5IBAAAAkJ5IBgAAAEB6IhkAAAAA6YlkAAAAAKQ3qEi2Zs2amDp1atTU1ERjY2Ns27btpGvXrVsXs2fPjnHjxsW4ceOiubn5HdcDADB8zHkAQFYlR7INGzZES0tLtLa2xo4dO2LatGkxZ86cePPNN0+4fsuWLXHDDTfED37wg9i6dWs0NDTEJz7xifj5z39+2psHAODMMecBAJlVFEVRlHJDY2NjXHnllfHwww9HRERfX180NDTEbbfdFsuWLXvX+3t7e2PcuHHx8MMPx8KFC0/pObu7u6Ouri66urqitra2lO0CAEmZH0pnzgMARoNyzQ8lvZKsp6cntm/fHs3Nzb//AmPGRHNzc2zduvWUvsZbb70Vb7/9dpx//vknXXP06NHo7u4e8AAAoHzMeQBAdiVFsoMHD0Zvb2/U19cPuF5fXx8dHR2n9DXuuOOOmDx58oAB7A+1tbVFXV1d/6OhoaGUbQIAUCJzHgCQ3ZB+uuWqVati/fr18cwzz0RNTc1J1y1fvjy6urr6H/v37x/CXQIAUCpzHgAw2o0tZfH48eOjsrIyOjs7B1zv7OyMiRMnvuO9DzzwQKxatSq+//3vx+WXX/6Oa6urq6O6urqUrQEAcBrMeQBAdiW9kqyqqipmzJgR7e3t/df6+vqivb09mpqaTnrf/fffH/fdd19s3rw5Zs6cOfjdAgBQFuY8ACC7kl5JFhHR0tISixYtipkzZ8asWbNi9erVceTIkVi8eHFERCxcuDCmTJkSbW1tERHxj//4j7FixYp48sknY+rUqf3vafG+970v3ve+953BbwUAgNNhzgMAMis5ks2fPz8OHDgQK1asiI6Ojpg+fXps3ry5/01e9+3bF2PG/P4Fal//+tejp6cn/uqv/mrA12ltbY0vfvGLp7d7AADOGHMeAJBZRVEUxXBv4t10d3dHXV1ddHV1RW1t7XBvBwAYBcwPo4NzAgBKVa75YUg/3RIAAAAARiKRDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACA9kQwAAACA9EQyAAAAANITyQAAAABITyQDAAAAID2RDAAAAID0RDIAAAAA0hPJAAAAAEhPJAMAAAAgPZEMAAAAgPREMgAAAADSE8kAAAAASE8kAwAAACC9QUWyNWvWxNSpU6OmpiYaGxtj27Zt77j+O9/5TlxyySVRU1MTl112WWzatGlQmwUAoLzMeQBAViVHsg0bNkRLS0u0trbGjh07Ytq0aTFnzpx48803T7j+xRdfjBtuuCFuuumm2LlzZ8ybNy/mzZsXP/nJT0578wAAnDnmPAAgs4qiKIpSbmhsbIwrr7wyHn744YiI6Ovri4aGhrjtttti2bJlx62fP39+HDlyJL73ve/1X/vYxz4W06dPj7Vr157Sc3Z3d0ddXV10dXVFbW1tKdsFAJIyP5TOnAcAjAblmh/GlrK4p6cntm/fHsuXL++/NmbMmGhubo6tW7ee8J6tW7dGS0vLgGtz5syJZ5999qTPc/To0Th69Gj/n7u6uiLit/8nAACcit/NDSX+PDAtcx4AMFqUa84rKZIdPHgwent7o76+fsD1+vr62LNnzwnv6ejoOOH6jo6Okz5PW1tb3Hvvvcddb2hoKGW7AADx3//931FXVzfc2xjxzHkAwGhzpue8kiLZUFm+fPmAn0oeOnQo3v/+98e+ffsMuSNUd3d3NDQ0xP79+/2qxAjmnEYH5zTyOaPRoaurKy644II4//zzh3sr/D/mvNHHv/NGB+c0Ojin0cE5jXzlmvNKimTjx4+PysrK6OzsHHC9s7MzJk6ceMJ7Jk6cWNL6iIjq6uqorq4+7npdXZ1/QEe42tpaZzQKOKfRwTmNfM5odBgzZlAf5p2OOY934995o4NzGh2c0+jgnEa+Mz3nlfTVqqqqYsaMGdHe3t5/ra+vL9rb26OpqemE9zQ1NQ1YHxHx/PPPn3Q9AABDz5wHAGRX8q9btrS0xKJFi2LmzJkxa9asWL16dRw5ciQWL14cERELFy6MKVOmRFtbW0RE3H777XHNNdfEgw8+GNdff32sX78+fvzjH8ejjz56Zr8TAABOizkPAMis5Eg2f/78OHDgQKxYsSI6Ojpi+vTpsXnz5v43bd23b9+Al7tdddVV8eSTT8bdd98dd955Z/zZn/1ZPPvss3HppZee8nNWV1dHa2vrCV+az8jgjEYH5zQ6OKeRzxmNDs6pdOY8TsQZjQ7OaXRwTqODcxr5ynVGFYXPRQcAAAAgOe9kCwAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQ3oiJZGvWrImpU6dGTU1NNDY2xrZt295x/Xe+85245JJLoqamJi677LLYtGnTEO00r1LOaN26dTF79uwYN25cjBs3Lpqbm9/1TDkzSv279Dvr16+PioqKmDdvXnk3SESUfk6HDh2KJUuWxKRJk6K6ujouvvhi/94rs1LPaPXq1fHBD34wzj777GhoaIilS5fGb37zmyHabU4//OEPY+7cuTF58uSoqKiIZ5999l3v2bJlS3z0ox+N6urq+MAHPhBPPPFE2feJOW80MOeNDua80cGcN/KZ80a+YZvzihFg/fr1RVVVVfH4448X//mf/1ncfPPNxXnnnVd0dnaecP2PfvSjorKysrj//vuLl19+ubj77ruLs846q3jppZeGeOd5lHpGN954Y7FmzZpi586dxe7du4u/+Zu/Kerq6or/+q//GuKd51LqOf3O66+/XkyZMqWYPXt28Zd/+ZdDs9nESj2no0ePFjNnziyuu+664oUXXihef/31YsuWLcWuXbuGeOd5lHpG3/rWt4rq6uriW9/6VvH6668Xzz33XDFp0qRi6dKlQ7zzXDZt2lTcddddxdNPP11ERPHMM8+84/q9e/cW55xzTtHS0lK8/PLLxde+9rWisrKy2Lx589BsOClz3shnzhsdzHmjgzlv5DPnjQ7DNeeNiEg2a9asYsmSJf1/7u3tLSZPnly0tbWdcP2nPvWp4vrrrx9wrbGxsfjbv/3bsu4zs1LP6A8dO3asOPfcc4tvfvOb5doixeDO6dixY8VVV11VfOMb3ygWLVpkeBoCpZ7T17/+9eLCCy8senp6hmqL6ZV6RkuWLCn+/M//fMC1lpaW4uqrry7rPvm9UxmevvCFLxQf+chHBlybP39+MWfOnDLuDHPeyGfOGx3MeaODOW/kM+eNPkM55w37r1v29PTE9u3bo7m5uf/amDFjorm5ObZu3XrCe7Zu3TpgfUTEnDlzTrqe0zOYM/pDb731Vrz99ttx/vnnl2ub6Q32nL70pS/FhAkT4qabbhqKbaY3mHP67ne/G01NTbFkyZKor6+PSy+9NFauXBm9vb1Dte1UBnNGV111VWzfvr3/pfp79+6NTZs2xXXXXTcke+bUmB+Gnjlv5DPnjQ7mvNHBnDfymfPeu87U/DD2TG5qMA4ePBi9vb1RX18/4Hp9fX3s2bPnhPd0dHSccH1HR0fZ9pnZYM7oD91xxx0xefLk4/6h5cwZzDm98MIL8dhjj8WuXbuGYIdEDO6c9u7dG//+7/8en/70p2PTpk3x2muvxec+97l4++23o7W1dSi2ncpgzujGG2+MgwcPxsc//vEoiiKOHTsWt956a9x5551DsWVO0cnmh+7u7vj1r38dZ5999jDt7L3LnDfymfNGB3Pe6GDOG/nMee9dZ2rOG/ZXkvHet2rVqli/fn0888wzUVNTM9zb4f8cPnw4FixYEOvWrYvx48cP93Z4B319fTFhwoR49NFHY8aMGTF//vy46667Yu3atcO9Nf7Pli1bYuXKlfHII4/Ejh074umnn46NGzfGfffdN9xbAygrc97IZM4bPcx5I585L5dhfyXZ+PHjo7KyMjo7Owdc7+zsjIkTJ57wnokTJ5a0ntMzmDP6nQceeCBWrVoV3//+9+Pyyy8v5zbTK/WcfvrTn8Ybb7wRc+fO7b/W19cXERFjx46NV155JS666KLybjqhwfx9mjRpUpx11llRWVnZf+1DH/pQdHR0RE9PT1RVVZV1z9kM5ozuueeeWLBgQXzmM5+JiIjLLrssjhw5ErfcckvcddddMWaMn0mNBCebH2pra72KrEzMeSOfOW90MOeNDua8kc+c9951pua8YT/NqqqqmDFjRrS3t/df6+vri/b29mhqajrhPU1NTQPWR0Q8//zzJ13P6RnMGUVE3H///XHffffF5s2bY+bMmUOx1dRKPadLLrkkXnrppdi1a1f/45Of/GRce+21sWvXrmhoaBjK7acxmL9PV199dbz22mv9w21ExKuvvhqTJk0yOJXBYM7orbfeOm5A+t2w+9v3GmUkMD8MPXPeyGfOGx3MeaODOW/kM+e9d52x+aGkt/kvk/Xr1xfV1dXFE088Ubz88svFLbfcUpx33nlFR0dHURRFsWDBgmLZsmX963/0ox8VY8eOLR544IFi9+7dRWtrq48GL7NSz2jVqlVFVVVV8dRTTxW/+MUv+h+HDx8erm8hhVLP6Q/51KOhUeo57du3rzj33HOLv/u7vyteeeWV4nvf+14xYcKE4stf/vJwfQvveaWeUWtra3HuuecW//Iv/1Ls3bu3+Ld/+7fioosuKj71qU8N17eQwuHDh4udO3cWO3fuLCKieOihh4qdO3cWP/vZz4qiKIply5YVCxYs6F//u48G/4d/+Idi9+7dxZo1awb10eCUxpw38pnzRgdz3uhgzhv5zHmjw3DNeSMikhVFUXzta18rLrjggqKqqqqYNWtW8R//8R/9/9s111xTLFq0aMD6b3/728XFF19cVFVVFR/5yEeKjRs3DvGO8ynljN7//vcXEXHco7W1deg3nkypf5f+P8PT0Cn1nF588cWisbGxqK6uLi688MLiK1/5SnHs2LEh3nUupZzR22+/XXzxi18sLrrooqKmpqZoaGgoPve5zxX/8z//M/QbT+QHP/jBCf9b87uzWbRoUXHNNdccd8/06dOLqqqq4sILLyz+6Z/+acj3nZE5b+Qz540O5rzRwZw38pnzRr7hmvMqisLrAwEAAADIbdjfkwwAAAAAhptIBgAAAEB6IhkAAAAA6YlkAAAAAKQnkgEAAACQnkgGAAAAQHoiGQAAAADpiWQAAAAApCeSAQAAAJCeSAYAAABAeiIZAAAAAOmJZAAAAACk978//A8cDMwb6QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GhostsEnv()\n",
    "agent = GhostsAgent(env)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 4000\n",
    "agent.train(episodes=num_episodes)\n",
    "\n",
    "# Visualize the training results\n",
    "agent.visualize_training()\n",
    "\n",
    "# Evaluate the agent\n",
    "eval_episodes = 100\n",
    "agent.evaluate(episodes=eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game 1\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 0, Reward: 10\n",
      ". G . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 3, Reward: 10\n",
      ". G . . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 1, Reward: 10\n",
      ". G G . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 2, Reward: 10\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 5, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . B . . .\n",
      "\n",
      "Action: 4, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . . .\n",
      "\n",
      "Action: 7, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . B .\n",
      "\n",
      "Action: 6, Reward: 19\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Action: 2, Reward: 99\n",
      "G . G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Game Over! Total Reward: 173\n",
      "\n",
      "Game 2\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 0, Reward: 10\n",
      ". G . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 3, Reward: 10\n",
      ". G . . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 1, Reward: 10\n",
      ". G G . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 2, Reward: 10\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 5, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . B . . .\n",
      "\n",
      "Action: 4, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . . .\n",
      "\n",
      "Action: 7, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . B .\n",
      "\n",
      "Action: 6, Reward: 19\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Action: 2, Reward: 99\n",
      "G . G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Game Over! Total Reward: 173\n",
      "\n",
      "Game 3\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 0, Reward: 10\n",
      ". G . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 3, Reward: 10\n",
      ". G . . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 1, Reward: 10\n",
      ". G G . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 2, Reward: 10\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 5, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . B . . .\n",
      "\n",
      "Action: 4, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . . .\n",
      "\n",
      "Action: 7, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . B .\n",
      "\n",
      "Action: 6, Reward: 19\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Action: 2, Reward: 99\n",
      "G . G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Game Over! Total Reward: 173\n",
      "\n",
      "Game 4\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 0, Reward: 10\n",
      ". G . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 3, Reward: 10\n",
      ". G . . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 1, Reward: 10\n",
      ". G G . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 2, Reward: 10\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 5, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . B . . .\n",
      "\n",
      "Action: 4, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . . .\n",
      "\n",
      "Action: 7, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . B .\n",
      "\n",
      "Action: 6, Reward: 19\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Action: 2, Reward: 99\n",
      "G . G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Game Over! Total Reward: 173\n",
      "\n",
      "Game 5\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 0, Reward: 10\n",
      ". G . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 3, Reward: 10\n",
      ". G . . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 1, Reward: 10\n",
      ". G G . G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 2, Reward: 10\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "\n",
      "Action: 5, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . B . . .\n",
      "\n",
      "Action: 4, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . . .\n",
      "\n",
      "Action: 7, Reward: 5\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B . B .\n",
      "\n",
      "Action: 6, Reward: 19\n",
      ". G G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Action: 2, Reward: 99\n",
      "G . G G G .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". B B B B .\n",
      "\n",
      "Game Over! Total Reward: 173\n",
      "\n",
      "Average Reward over 5 games: 173.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Play games\n",
    "agent.play_games(num_games=5, human_player=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lucac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:661\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 661\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n",
      "File \u001b[1;32mc:\\Users\\lucac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:620\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(start_server)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(start_server)\n\u001b[0;32m     62\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[1;32mc:\\Users\\lucac\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\selector_events.py:101\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "async def game_loop(websocket, path):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    player = 0  # Human player starts\n",
    "\n",
    "    while not done:\n",
    "        print(f\"Player {player}'s turn.\")\n",
    "        \n",
    "        if player == 0:  # Human player's turn\n",
    "            legal_actions = env.legal_placement_actions(player) if env.phase == 0 else env.legal_movement_actions(player)\n",
    "            print(f\"Legal actions for player {player}: {legal_actions}\")\n",
    "            \n",
    "            # Send legal actions to Unity\n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"legal_actions\",\n",
    "                \"actions\": [{\"action\": action[0], \"x\": action[1], \"y\": action[2]} for action in legal_actions]\n",
    "            }))\n",
    "\n",
    "            # Receive action from Unity\n",
    "            response = await websocket.recv()\n",
    "            action_data = json.loads(response)\n",
    "            action = (action_data[\"action\"], action_data[\"x\"], action_data[\"y\"], player)\n",
    "            print(f\"Received action from Unity: {action}\")\n",
    "        else:  # AI's turn\n",
    "            action = agent.select_action(state, player)\n",
    "            print(f\"AI action: {action}\")\n",
    "\n",
    "        # Perform the action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        print(f\"Action performed. Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        # Send the AI's action to Unity if it's the AI's turn\n",
    "        if player == 1:\n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"ai_action\",\n",
    "                \"action\": {\"action\": action[0], \"x\": action[1], \"y\": action[2]}\n",
    "            }))\n",
    "            print(f\"Sent AI action to Unity: {action}\")\n",
    "\n",
    "        state = next_state\n",
    "        player = 1 - player  # Switch players\n",
    "\n",
    "    # Game over, send result to Unity\n",
    "    await websocket.send(json.dumps({\n",
    "        \"type\": \"game_over\",\n",
    "        \"winner\": \"Human\" if done == 0 else \"AI\" if done == 1 else \"Draw\"\n",
    "    }))\n",
    "    print(\"Game over. Result sent to Unity.\")\n",
    "\n",
    "def main():\n",
    "    manager = ClientManager()\n",
    "    start_server = websockets.serve(manager.handle_connection, \"0.0.0.0\", 8765)\n",
    "    event_loop = asyncio.get_event_loop()\n",
    "    event_loop.run_until_complete(start_server)\n",
    "    event_loop.run_forever()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
