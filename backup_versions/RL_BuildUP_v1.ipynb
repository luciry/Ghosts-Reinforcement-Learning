{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a reinforcement learning project to simulate the board game Ghosts by Alex Randolph involves several steps. This guide will cover the rules and logic of the game, and then outline a step-by-step approach to implementing a reinforcement learning agent using Gym and Q-Learning.\n",
    "Game Rules and Logic\n",
    "Game Setup:\n",
    "The game is played on a 6x6 board.\n",
    "Each player has 8 ghosts: 4 good ghosts and 4 evil ghosts. The identity of each ghost is hidden from the opponent.\n",
    "Players arrange their ghosts in the back two rows on their side of the board.\n",
    "Game Objective:\n",
    "A player wins by either:\n",
    "Capturing all four of the opponent's good ghosts.\n",
    "Having the opponent capture all four of their evil ghosts.\n",
    "Moving one of their good ghosts to one of the opponent's corner exits.\n",
    "Game Play:\n",
    "Players take turns moving one ghost per turn.\n",
    "Ghosts can move one square in any direction (up, down, left, or right) but not diagonally.\n",
    "A ghost can capture an opponent's ghost by moving into its square, revealing the captured ghost's identity.\n",
    "Reinforcement Learning Project\n",
    "Step 1: Environment Setup\n",
    "Define the State Space:\n",
    "The state can be represented as a 6x6 grid where each cell can be empty, contain a good ghost, or contain an evil ghost.\n",
    "Include additional information to track which ghosts belong to which player and their identities.\n",
    "Define the Action Space:\n",
    "Actions include moving any ghost to an adjacent square (up, down, left, right).\n",
    "Rewards:\n",
    "Positive reward for capturing an opponent's good ghost or moving a good ghost to an exit.\n",
    "Negative reward for capturing an evil ghost or losing a good ghost.\n",
    "Step 2: Implement the Game Logic\n",
    "Create a Python class to simulate the game board and enforce the rules.\n",
    "Implement methods to initialize the board, make moves, check for game termination, and calculate rewards.\n",
    "Step 3: Create a Gym Environment\n",
    "Use OpenAI Gym to create a custom environment for the game.\n",
    "Implement the necessary methods: reset(), step(action), render(), and close().\n",
    "Step 4: Implement Q-Learning\n",
    "Initialize Q-Table:\n",
    "Use a dictionary or a large array to store Q-values for state-action pairs.\n",
    "Define Hyperparameters:\n",
    "Learning rate (\n",
    "α\n",
    "α), discount factor (\n",
    "γ\n",
    "γ), and exploration rate (\n",
    "ϵ\n",
    "ϵ).\n",
    "Training Loop:\n",
    "For each episode, reset the environment.\n",
    "For each step in the episode:\n",
    "Choose an action using an epsilon-greedy policy.\n",
    "Execute the action and observe the reward and next state.\n",
    "Update the Q-value using the Q-learning formula:\n",
    "Q\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "=\n",
    "Q\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "+\n",
    "α\n",
    "[\n",
    "r\n",
    "+\n",
    "γ\n",
    "max\n",
    "⁡\n",
    "a\n",
    "′\n",
    "Q\n",
    "(\n",
    "s\n",
    "′\n",
    ",\n",
    "a\n",
    "′\n",
    ")\n",
    "−\n",
    "Q\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    "]\n",
    "Q(s,a)=Q(s,a)+α[r+γmax \n",
    "a \n",
    "′\n",
    " \n",
    "​\n",
    " Q(s \n",
    "′\n",
    " ,a \n",
    "′\n",
    " )−Q(s,a)]\n",
    "Update the state.\n",
    "Reduce the exploration rate over time.\n",
    "Step 5: Evaluate the Agent\n",
    "Test the trained agent against a random or heuristic-based opponent.\n",
    "Evaluate its performance based on win/loss ratio and the ability to learn optimal strategies.\n",
    "Step 6: Optimize and Experiment\n",
    "Experiment with different hyperparameters and strategies.\n",
    "Consider using more advanced techniques like Deep Q-Learning if the state space is too large.\n",
    "By following these steps, you can create a reinforcement learning agent capable of playing the game Ghosts. This project will help you understand the intricacies of game AI and the application of reinforcement learning techniques in imperfect information games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "PLACE_BLUE = 4\n",
    "PLACE_RED = 5\n",
    "ACTIONS = [UP, RIGHT, DOWN, LEFT, PLACE_BLUE, PLACE_RED]\n",
    "\n",
    "# Space\n",
    "EMPTY = 0\n",
    "PLAYER_BLUE = 1\n",
    "PLAYER_RED = 2\n",
    "OPPONENT_BLUE = 3\n",
    "OPPONENT_RED = 4\n",
    "\n",
    "# TWO PLAYERS GAME\n",
    "\n",
    "# Define mappings from action indices to names\n",
    "ACTION_NAMES = {\n",
    "    UP: \"Move Up\",\n",
    "    RIGHT: \"Move Right\",\n",
    "    DOWN: \"Move Down\",\n",
    "    LEFT: \"Move Left\",\n",
    "    PLACE_BLUE: \"Place Blue Piece\",\n",
    "    PLACE_RED: \"Place Red Piece\"\n",
    "}\n",
    "\n",
    "# Define mappings from space values to names\n",
    "SPACE_NAMES = {\n",
    "    EMPTY: \"Empty\",\n",
    "    PLAYER_BLUE: \"Player Blue\",\n",
    "    PLAYER_RED: \"Player Red\",\n",
    "    OPPONENT_BLUE: \"Opponent Blue\",\n",
    "    OPPONENT_RED: \"Opponent Red\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "# Constants for ghost types and actions\n",
    "EMPTY = 0\n",
    "PLAYER_BLUE = 1\n",
    "PLAYER_RED = 2\n",
    "OPPONENT_BLUE = 3\n",
    "OPPONENT_RED = 4\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class GhostsEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GhostsEnv, self).__init__()\n",
    "\n",
    "        # Size of the board is size x size\n",
    "        self.size = 6\n",
    "\n",
    "        # Action space is tuple: (action, x, y, player)\n",
    "        # action: 0-3 (move up, right, down, left), 4 (capture), 5 (place)\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(6), spaces.Discrete(self.size), spaces.Discrete(self.size), spaces.Discrete(2)))\n",
    "\n",
    "        # Observation space is a 6x6x4 matrix\n",
    "        self.observation_space = spaces.Box(low=0, high=6, shape=(self.size, self.size, 4), dtype=int)\n",
    "\n",
    "        # Reward\n",
    "        self.reward = 0\n",
    "\n",
    "        # Initialize the game state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize the board\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "\n",
    "        # Reset phase (0: placement, 1: movement)\n",
    "        self.phase = 0\n",
    "\n",
    "        # Reset ghost counts\n",
    "        self.player_blue_count = 0\n",
    "        self.opponent_blue_count = 0\n",
    "        self.player_red_count = 0\n",
    "        self.opponent_red_count = 0\n",
    "\n",
    "        return self.board\n",
    "    \n",
    "    def legal_placement_actions(self, player):\n",
    "        actions = []\n",
    "        if player == 0:  # Player's turn\n",
    "            rows = [4, 5]\n",
    "            valid_cells = [(x, y) for x in rows for y in range(1, self.size - 1) if self.board[x, y] == 0]\n",
    "        else:  # Opponent's turn\n",
    "            rows = [0, 1]\n",
    "            valid_cells = [(x, y) for x in rows for y in range(1, self.size - 1) if self.board[x, y] == 0]\n",
    "\n",
    "        if player == 0:\n",
    "            if self.player_blue_count < 4:\n",
    "                for x, y in valid_cells:\n",
    "                    actions.append((4, x, y, player))  # Place blue ghost\n",
    "            if self.player_red_count < 4:\n",
    "                for x, y in valid_cells:\n",
    "                    actions.append((5, x, y, player))  # Place red ghost\n",
    "        else:\n",
    "            if self.opponent_blue_count < 4:\n",
    "                for x, y in valid_cells:\n",
    "                    actions.append((4, x, y, player))  # Place blue ghost\n",
    "            if self.opponent_red_count < 4:\n",
    "                for x, y in valid_cells:\n",
    "                    actions.append((5, x, y, player))  # Place red ghost\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def legal_movement_actions(self, player):\n",
    "        actions = []\n",
    "        \n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if (player == 0 and self.board[x, y] in [1, 2]) or (player == 1 and self.board[x, y] in [3, 4]):\n",
    "                    # Check all four directions\n",
    "                    if x > 0 and self.board[x-1, y] == EMPTY:  # Move left\n",
    "                        actions.append((LEFT, x, y, player))\n",
    "                    if x < self.size - 1 and self.board[x+1, y] == EMPTY:  # Move right\n",
    "                        actions.append((RIGHT, x, y, player))\n",
    "                    if y > 0 and self.board[x, y-1] == EMPTY:  # Move up\n",
    "                        actions.append((UP, x, y, player))\n",
    "                    if y < self.size - 1 and self.board[x, y+1] == EMPTY:  # Move down\n",
    "                        actions.append((DOWN, x, y, player))\n",
    "\n",
    "                    # Capture opponent's ghost\n",
    "                    if x > 0 and player == 0 and self.board[x-1, y] in [3, 4]:  # Capture left\n",
    "                        actions.append((LEFT, x, y, player))\n",
    "                    if x < self.size - 1 and player == 0 and self.board[x+1, y] in [3, 4]:  # Capture right\n",
    "                        actions.append((RIGHT, x, y, player))\n",
    "                    if y > 0 and player == 0 and self.board[x, y-1] in [3, 4]:  # Capture up\n",
    "                        actions.append((UP, x, y, player))\n",
    "                    if y < self.size - 1 and player == 0 and self.board[x, y+1] in [3, 4]:  # Capture down\n",
    "                        actions.append((DOWN, x, y, player))\n",
    "        \n",
    "        return actions\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        if self.phase == 0:\n",
    "            self._place(action)\n",
    "        else:\n",
    "            self._take_action(action)\n",
    "\n",
    "        return self.board, self._get_reward(), self._is_done(), {}\n",
    "\n",
    "    def _place(self, action):\n",
    "        action, x, y, player = action\n",
    "        if player == 0:\n",
    "            if action == 4:\n",
    "                self.board[x, y] = PLAYER_BLUE\n",
    "                self.player_blue_count += 1\n",
    "            elif action == 5:\n",
    "                self.board[x, y] = PLAYER_RED\n",
    "                self.player_red_count += 1\n",
    "        else:\n",
    "            if action == 4:\n",
    "                self.board[x, y] = OPPONENT_BLUE\n",
    "                self.opponent_blue_count += 1\n",
    "            elif action == 5:\n",
    "                self.board[x, y] = OPPONENT_RED\n",
    "                self.opponent_red_count += 1\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        action, x, y, player = action\n",
    "        if player == 0:\n",
    "            if action == UP:\n",
    "                self._move(x, y, x, y-1)\n",
    "            elif action == RIGHT:\n",
    "                self._move(x, y, x+1, y)\n",
    "            elif action == DOWN:\n",
    "                self._move(x, y, x, y+1)\n",
    "            elif action == LEFT:\n",
    "                self._move(x, y, x-1, y)\n",
    "        else:\n",
    "            if action == UP:\n",
    "                self._move(x, y, x, y-1)\n",
    "            elif action == RIGHT:\n",
    "                self._move(x, y, x+1, y)\n",
    "            elif action == DOWN:\n",
    "                self._move(x, y, x, y+1)\n",
    "            elif action == LEFT:\n",
    "                self._move(x, y, x-1, y)\n",
    "\n",
    "    def _move(self, x, y, new_x, new_y):\n",
    "        if new_x >= 0 and new_x < self.size and new_y >= 0 and new_y < self.size:\n",
    "            if self.board[new_x, new_y] == EMPTY:\n",
    "                self.board[new_x, new_y] = self.board[x, y]\n",
    "                self.board[x, y] = EMPTY\n",
    "            elif self.board[new_x, new_y] == OPPONENT_BLUE:\n",
    "                self.board[new_x, new_y] = PLAYER_BLUE\n",
    "                self.board[x, y] = EMPTY\n",
    "                self.reward = 10\n",
    "                self.opponent_blue_count -= 1\n",
    "            elif self.board[new_x, new_y] == OPPONENT_RED:\n",
    "                self.board[new_x, new_y] = PLAYER_RED\n",
    "                self.board[x, y] = EMPTY\n",
    "                self.reward = -10\n",
    "                self.opponent_red_count -= 1\n",
    "            elif self.board[new_x, new_y] == PLAYER_BLUE:\n",
    "                self.board[new_x, new_y] = PLAYER_BLUE\n",
    "                self.board[x, y] = EMPTY\n",
    "                self.reward = -10\n",
    "                self.player_blue_count -= 1\n",
    "            elif self.board[new_x, new_y] == PLAYER_RED:\n",
    "                self.board[new_x, new_y] = PLAYER_RED\n",
    "                self.board[x, y] = EMPTY\n",
    "                self.reward = 10\n",
    "                self.player_red_count -= 1\n",
    "\n",
    "    def _get_reward(self):\n",
    "        if self.phase == 0:\n",
    "            return 0  # No reward during placement phase\n",
    "        \n",
    "        # Reward for capturing opponent's ghosts\n",
    "        if self.reward == 10:\n",
    "            return 1\n",
    "        elif self.reward == -10:\n",
    "            return -1\n",
    "        \n",
    "        # Small negative reward for each move to encourage efficiency\n",
    "        return -0.01\n",
    "    \n",
    "    def _is_done(self):\n",
    "        if self.phase == 0:\n",
    "            if self.player_blue_count == 4 and self.player_red_count == 4 and self.opponent_blue_count == 4 and self.opponent_red_count == 4:\n",
    "                self.phase = 1\n",
    "                return False\n",
    "            return False\n",
    "        else:\n",
    "            if self.player_blue_count == 0 or self.opponent_red_count == 0:\n",
    "                self.reward = -100\n",
    "                return True\n",
    "            elif self.player_red_count == 0 or self.opponent_blue_count == 0:\n",
    "                self.reward = 100\n",
    "                return True\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        for i in range(self.size):\n",
    "            print(\"|\", end=\"\")\n",
    "            for j in range(self.size):\n",
    "                print(\" \", self.board[i, j], end=\" \")\n",
    "            print(\"|\")\n",
    "        print()\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Box' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m env \u001b[38;5;241m=\u001b[39m GhostsEnv()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Initialize the Q-learning agent\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mQLearningAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m    111\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[128], line 5\u001b[0m, in \u001b[0;36mQLearningAgent.__init__\u001b[1;34m(self, env, alpha, gamma, epsilon)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 5\u001b[0m         \u001b[43m{\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m     ]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m alpha  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma  \u001b[38;5;66;03m# Discount factor\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Box' object is not iterable"
     ]
    }
   ],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.q_table = [\n",
    "            np.zeros((env.size, env.size, 6, 2)),  # Player 0's Q-table\n",
    "            np.zeros((env.size, env.size, 6, 2))   # Player 1's Q-table\n",
    "        ]\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "\n",
    "    def select_action(self, state, player):\n",
    "        # Debug: Print the current state\n",
    "        print(f\"\\nCurrent state for player {player}:\")\n",
    "        self.env.render()\n",
    "\n",
    "        # Determine legal actions\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration\n",
    "            legal_actions = self.env.legal_placement_actions(player) if self.env.phase == 0 else self.env.legal_movement_actions(player)\n",
    "        else:\n",
    "            # Exploitation\n",
    "            legal_actions = self.env.legal_placement_actions(player) if self.env.phase == 0 else self.env.legal_movement_actions(player)\n",
    "            \n",
    "        # Debug: Print the legal actions available\n",
    "        print(f\"Legal actions for player {player}: {legal_actions}\")\n",
    "\n",
    "        if not legal_actions:\n",
    "            raise ValueError(f\"No legal actions available for player {player} in state: {state}\")\n",
    "\n",
    "        # Check if we need to perform exploitation\n",
    "        if np.random.rand() >= self.epsilon and legal_actions:\n",
    "            action_q_values = []\n",
    "            for a in legal_actions:\n",
    "                action_index = a[0]  # Assuming action index is the first element of the tuple\n",
    "                if 0 <= action_index < self.q_table[player].shape[2]:  # Check if action index is within range\n",
    "                    q_value = self.q_table[player][state[0], state[1], action_index, player]\n",
    "                    action_q_values.append(np.max(q_value))\n",
    "                    print(f\"Q-value for action {a}: {q_value}\")\n",
    "                else:\n",
    "                    action_q_values.append(float('-inf'))\n",
    "\n",
    "            max_q_value_index = np.argmax(action_q_values)\n",
    "            action = legal_actions[max_q_value_index]\n",
    "        else:\n",
    "            action = legal_actions[np.random.choice(len(legal_actions))]\n",
    "\n",
    "        # Debug: Print the chosen action\n",
    "        action_name = ACTION_NAMES.get(action[0], \"Unknown Action\")\n",
    "        print(f\"Chosen action for player {player}: {action_name} (Raw action: {action})\")\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state, player):\n",
    "        q_value = self.q_table[player][state[0], state[1], action[0], player]\n",
    "        max_next_q_value = np.max(self.q_table[player][next_state[0], next_state[1], :, player])\n",
    "\n",
    "        # Update rule\n",
    "        self.q_table[player][state[0], state[1], action[0], player] = q_value + self.alpha * (reward + self.gamma * max_next_q_value - q_value)\n",
    "\n",
    "        # Debug: Print the Q-value update\n",
    "        print(f\"Updated Q-value for player {player}, state {state}, action {action}: {self.q_table[player][state[0], state[1], action[0], player]}\")\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            player = 0  # Player starts\n",
    "\n",
    "            print(f\"\\nStarting Episode {episode + 1}\")\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state, player)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.update(state, action, reward, next_state)\n",
    "\n",
    "                # Debug: Print state transition and reward\n",
    "                print(f\"Reward {reward}, done: {done}\")\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # Alternate between players\n",
    "                player = 1 - player\n",
    "\n",
    "                if done:\n",
    "                    self.env.render()\n",
    "                    print(f\"Game over. Reward: {reward}\")\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}: Training in progress...\")\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        player = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.select_action(state, player)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            player = 1 - player\n",
    "\n",
    "        print(\"Game over.\")\n",
    "\n",
    "# Initialize the environment\n",
    "env = GhostsEnv()\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current state for player 0:\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(4, 4, 1, 0), (4, 4, 2, 0), (4, 4, 3, 0), (4, 4, 4, 0), (4, 5, 1, 0), (4, 5, 2, 0), (4, 5, 3, 0), (4, 5, 4, 0), (5, 4, 1, 0), (5, 4, 2, 0), (5, 4, 3, 0), (5, 4, 4, 0), (5, 5, 1, 0), (5, 5, 2, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Q-value for action (4, 4, 1, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 2, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 3, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 4, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 1, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 2, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 3, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 4, 0): [4.16189042 4.16189042 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (5, 4, 1, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 2, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 3, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 4, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 1, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 2, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 3, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 4.06891146 4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Chosen action for player 0: Place Blue Piece (Raw action: (4, 4, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(4, 0, 1, 1), (4, 0, 2, 1), (4, 0, 3, 1), (4, 0, 4, 1), (4, 1, 1, 1), (4, 1, 2, 1), (4, 1, 3, 1), (4, 1, 4, 1), (5, 0, 1, 1), (5, 0, 2, 1), (5, 0, 3, 1), (5, 0, 4, 1), (5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1), (5, 1, 4, 1)]\n",
      "Q-value for action (4, 0, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Blue Piece (Raw action: (4, 0, 1, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(4, 4, 2, 0), (4, 4, 3, 0), (4, 4, 4, 0), (4, 5, 1, 0), (4, 5, 2, 0), (4, 5, 3, 0), (4, 5, 4, 0), (5, 4, 2, 0), (5, 4, 3, 0), (5, 4, 4, 0), (5, 5, 1, 0), (5, 5, 2, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Q-value for action (4, 4, 2, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 3, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 4, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 1, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 2, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 3, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 4, 0): [4.16189042 3.85167987 4.16189042 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (5, 4, 2, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 3, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 4, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 1, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 2, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 3, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 3.9851244  4.06891146 4.06891146 4.06891146 4.06891146]\n",
      "Chosen action for player 0: Place Blue Piece (Raw action: (4, 4, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(4, 0, 2, 1), (4, 0, 3, 1), (4, 0, 4, 1), (4, 1, 1, 1), (4, 1, 2, 1), (4, 1, 3, 1), (4, 1, 4, 1), (5, 0, 2, 1), (5, 0, 3, 1), (5, 0, 4, 1), (5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1), (5, 1, 4, 1)]\n",
      "Q-value for action (4, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Blue Piece (Raw action: (4, 0, 2, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(4, 4, 3, 0), (4, 4, 4, 0), (4, 5, 1, 0), (4, 5, 2, 0), (4, 5, 3, 0), (4, 5, 4, 0), (5, 4, 3, 0), (5, 4, 4, 0), (5, 5, 1, 0), (5, 5, 2, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Q-value for action (4, 4, 3, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 4, 4, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 1, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 2, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 3, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 4, 0): [4.16189042 3.85167987 3.85167987 4.16189042 4.16189042 4.16189042]\n",
      "Q-value for action (5, 4, 3, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 4, 4, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 1, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 2, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 3, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 3.9851244  3.9851244  4.06891146 4.06891146 4.06891146]\n",
      "Chosen action for player 0: Place Blue Piece (Raw action: (4, 4, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(4, 0, 3, 1), (4, 0, 4, 1), (4, 1, 1, 1), (4, 1, 2, 1), (4, 1, 3, 1), (4, 1, 4, 1), (5, 0, 3, 1), (5, 0, 4, 1), (5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1), (5, 1, 4, 1)]\n",
      "Q-value for action (4, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Blue Piece (Raw action: (4, 0, 3, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(4, 4, 4, 0), (4, 5, 1, 0), (4, 5, 2, 0), (4, 5, 3, 0), (4, 5, 4, 0), (5, 4, 4, 0), (5, 5, 1, 0), (5, 5, 2, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Q-value for action (4, 4, 4, 0): [4.16189042 3.85167987 3.85167987 3.85167987 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 1, 0): [4.16189042 3.85167987 3.85167987 3.85167987 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 2, 0): [4.16189042 3.85167987 3.85167987 3.85167987 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 3, 0): [4.16189042 3.85167987 3.85167987 3.85167987 4.16189042 4.16189042]\n",
      "Q-value for action (4, 5, 4, 0): [4.16189042 3.85167987 3.85167987 3.85167987 4.16189042 4.16189042]\n",
      "Q-value for action (5, 4, 4, 0): [4.06891146 3.9851244  3.9851244  3.9851244  4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 1, 0): [4.06891146 3.9851244  3.9851244  3.9851244  4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 2, 0): [4.06891146 3.9851244  3.9851244  3.9851244  4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 3, 0): [4.06891146 3.9851244  3.9851244  3.9851244  4.06891146 4.06891146]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 3.9851244  3.9851244  3.9851244  4.06891146 4.06891146]\n",
      "Chosen action for player 0: Place Blue Piece (Raw action: (4, 4, 4, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(4, 0, 4, 1), (4, 1, 1, 1), (4, 1, 2, 1), (4, 1, 3, 1), (4, 1, 4, 1), (5, 0, 4, 1), (5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1), (5, 1, 4, 1)]\n",
      "Q-value for action (4, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (4, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Blue Piece (Raw action: (4, 0, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(5, 5, 1, 0), (5, 5, 2, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Chosen action for player 0: Place Red Piece (Raw action: (5, 5, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   0   2   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1), (5, 1, 4, 1)]\n",
      "Chosen action for player 1: Place Red Piece (Raw action: (5, 1, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   0   2   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(5, 5, 1, 0), (5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Chosen action for player 0: Place Red Piece (Raw action: (5, 5, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   0   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(5, 1, 1, 1), (5, 1, 2, 1), (5, 1, 3, 1)]\n",
      "Chosen action for player 1: Place Red Piece (Raw action: (5, 1, 3, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   0   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(5, 5, 3, 0), (5, 5, 4, 0)]\n",
      "Q-value for action (5, 5, 3, 0): [4.06891146 3.9851244  3.9851244  2.76948188 2.76948188 4.06891146]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 3.9851244  3.9851244  2.76948188 2.76948188 4.06891146]\n",
      "Chosen action for player 0: Place Red Piece (Raw action: (5, 5, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   0   0   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   2   0   0 |\n",
      "\n",
      "Legal actions for player 1: [(5, 1, 1, 1), (5, 1, 2, 1)]\n",
      "Q-value for action (5, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Red Piece (Raw action: (5, 1, 1, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   4   0   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   2   0   0 |\n",
      "\n",
      "Legal actions for player 0: [(5, 5, 4, 0)]\n",
      "Q-value for action (5, 5, 4, 0): [4.06891146 2.76948188 3.9851244  2.76948188 2.76948188 4.06891146]\n",
      "Chosen action for player 0: Place Red Piece (Raw action: (5, 5, 4, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   4   0   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(5, 1, 2, 1)]\n",
      "Q-value for action (5, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Place Red Piece (Raw action: (5, 1, 2, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   4   4   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(3, 4, 1, 0), (0, 4, 1, 0), (3, 4, 2, 0), (3, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (0, 5, 1, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (3, 4, 1, 0): [7.37532757 6.98334147 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (0, 4, 1, 0): [3.11216933 3.3363122  3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (3, 4, 2, 0): [7.37532757 6.98334147 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (3, 4, 3, 0): [7.37532757 6.98334147 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 6.98334147 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.59994784 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 3.3363122  3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.59994784 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 4, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   3   3   3   3   0 |\n",
      "|  0   4   4   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(0, 0, 1, 1), (2, 0, 4, 1), (1, 1, 1, 1), (0, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1)]\n",
      "Q-value for action (0, 0, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Up (Raw action: (0, 0, 1, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  3   0   3   3   3   0 |\n",
      "|  0   4   4   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(3, 3, 1, 0), (1, 3, 1, 0), (0, 3, 1, 0), (2, 3, 1, 0), (3, 4, 2, 0), (0, 4, 2, 0), (3, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (3, 3, 1, 0): [3.26578654 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (1, 3, 1, 0): [0.76912636 1.88472731 0.85664835 0.85664835 0.85664835 1.56380538]\n",
      "Q-value for action (0, 3, 1, 0): [2.06645322 1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 3, 1, 0): [1.79727389 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Q-value for action (3, 4, 2, 0): [3.26578654 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (0, 4, 2, 0): [2.06645322 1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (3, 4, 3, 0): [3.26578654 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (3, 4, 4, 0): [3.26578654 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (2, 4, 4, 0): [1.79727389 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Q-value for action (3, 5, 1, 0): [3.26578654 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (0, 5, 1, 0): [2.06645322 1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 5, 4, 0): [1.79727389 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 3, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  3   0   3   3   3   0 |\n",
      "|  0   4   4   4   4   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(1, 0, 0, 1), (2, 0, 0, 1), (0, 0, 2, 1), (2, 0, 4, 1), (3, 1, 1, 1), (0, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1)]\n",
      "Q-value for action (1, 0, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 0, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 1, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 0, 0, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   0   3   3   3   0 |\n",
      "|  3   4   4   4   4   0 |\n",
      "|  0   1   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 2, 1, 0), (0, 2, 1, 0), (2, 2, 1, 0), (3, 2, 1, 0), (3, 4, 2, 0), (0, 4, 2, 0), (3, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 2, 1, 0): [1.73349974 1.88472731 0.85664835 0.85664835 0.85664835 1.56380538]\n",
      "Q-value for action (0, 2, 1, 0): [1.8395128  1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 2, 1, 0): [1.85617231 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Q-value for action (3, 2, 1, 0): [3.51996765 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (3, 4, 2, 0): [3.51996765 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (0, 4, 2, 0): [1.8395128  1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (3, 4, 3, 0): [3.51996765 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (3, 4, 4, 0): [3.51996765 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (2, 4, 4, 0): [1.85617231 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Q-value for action (3, 5, 1, 0): [3.51996765 5.17899194 6.98334147 6.98334147 6.98334147 7.37532757]\n",
      "Q-value for action (0, 5, 1, 0): [1.8395128  1.85384448 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 5, 4, 0): [1.85617231 2.22462123 1.59994784 1.59994784 1.59994784 2.6247102 ]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 2, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   0   3   3   3   0 |\n",
      "|  3   2   4   4   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(0, 0, 2, 1), (2, 0, 4, 1), (3, 1, 0, 1), (1, 1, 0, 1), (1, 1, 2, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1)]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 1, 0, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   0   3   3   3   0 |\n",
      "|  0   2   4   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(3, 1, 1, 0), (1, 1, 1, 0), (0, 1, 1, 0), (2, 1, 1, 0), (3, 4, 2, 0), (0, 4, 2, 0), (3, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (3, 1, 1, 0): [ 7.37532757 -3.68730335  6.98334147  6.98334147  6.98334147  7.37532757]\n",
      "Q-value for action (1, 1, 1, 0): [ 1.56380538 -0.0278649   0.85664835  0.85664835  0.85664835  1.56380538]\n",
      "Q-value for action (0, 1, 1, 0): [3.11216933 1.34508139 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 1, 1, 0): [ 2.6247102  -0.0823117   1.59994784  1.59994784  1.59994784  2.6247102 ]\n",
      "Q-value for action (3, 4, 2, 0): [ 7.37532757 -3.68730335  6.98334147  6.98334147  6.98334147  7.37532757]\n",
      "Q-value for action (0, 4, 2, 0): [3.11216933 1.34508139 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (3, 4, 3, 0): [ 7.37532757 -3.68730335  6.98334147  6.98334147  6.98334147  7.37532757]\n",
      "Q-value for action (3, 4, 4, 0): [ 7.37532757 -3.68730335  6.98334147  6.98334147  6.98334147  7.37532757]\n",
      "Q-value for action (2, 4, 4, 0): [ 2.6247102  -0.0823117   1.59994784  1.59994784  1.59994784  2.6247102 ]\n",
      "Q-value for action (3, 5, 1, 0): [ 7.37532757 -3.68730335  6.98334147  6.98334147  6.98334147  7.37532757]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.34508139 3.3363122  3.3363122  3.3363122  3.11216933]\n",
      "Q-value for action (2, 5, 4, 0): [ 2.6247102  -0.0823117   1.59994784  1.59994784  1.59994784  2.6247102 ]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 1, 1, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   3   3   3   0 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(2, 0, 4, 1), (1, 1, 2, 1), (0, 1, 2, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Q-value for action (2, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Down (Raw action: (2, 0, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   3   3   0   3 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   1   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (2, 0, 1, 0), (3, 4, 2, 0), (0, 4, 2, 0), (3, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 0.85664835 0.85664835 1.88472731 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 3.3363122  3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (2, 0, 1, 0): [2.6247102  1.54804802 1.59994784 1.59994784 2.22462123 1.79727389]\n",
      "Q-value for action (3, 4, 2, 0): [7.37532757 2.01794195 6.98334147 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (0, 4, 2, 0): [3.11216933 1.68498238 3.3363122  3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (3, 4, 3, 0): [7.37532757 2.01794195 6.98334147 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 6.98334147 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.59994784 1.59994784 2.22462123 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 6.98334147 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 3.3363122  3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.59994784 1.59994784 2.22462123 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 4, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   3   3   0   3 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   1   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(2, 0, 3, 1), (1, 0, 5, 1), (0, 0, 5, 1), (1, 1, 2, 1), (0, 1, 2, 1), (1, 1, 3, 1), (3, 1, 4, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Q-value for action (2, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Down (Raw action: (2, 0, 3, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   3   0   3   3 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   1   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (2, 0, 1, 0), (3, 3, 2, 0), (1, 3, 2, 0), (0, 3, 2, 0), (2, 3, 2, 0), (3, 4, 3, 0), (0, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 0.85664835 1.88472731 0.85664835 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 3.3363122  1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (2, 0, 1, 0): [2.6247102  1.54804802 1.59994784 2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (3, 3, 2, 0): [7.37532757 2.01794195 6.98334147 5.17899194 6.98334147 3.26578654]\n",
      "Q-value for action (1, 3, 2, 0): [1.56380538 1.27209434 0.85664835 1.88472731 0.85664835 0.76912636]\n",
      "Q-value for action (0, 3, 2, 0): [3.11216933 1.68498238 3.3363122  1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (2, 3, 2, 0): [2.6247102  1.54804802 1.59994784 2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (3, 4, 3, 0): [7.37532757 2.01794195 6.98334147 5.17899194 6.98334147 3.26578654]\n",
      "Q-value for action (0, 4, 3, 0): [3.11216933 1.68498238 3.3363122  1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 6.98334147 5.17899194 6.98334147 3.26578654]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.59994784 2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 6.98334147 5.17899194 6.98334147 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 3.3363122  1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 6.98334147 5.17899194 6.98334147 3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.59994784 2.22462123 1.59994784 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 3, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   3   0   3   3 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   1   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(2, 0, 2, 1), (0, 0, 4, 1), (1, 0, 5, 1), (0, 1, 2, 1), (3, 1, 3, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Q-value for action (2, 0, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 2, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Down (Raw action: (2, 0, 2, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   0   3   3   3 |\n",
      "|  0   0   4   4   4   0 |\n",
      "|  3   0   1   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (2, 0, 1, 0), (1, 2, 2, 0), (0, 2, 2, 0), (2, 2, 2, 0), (3, 2, 2, 0), (3, 4, 3, 0), (0, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.88472731 0.85664835 0.85664835 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 1.85384448 3.3363122  3.3363122  2.06645322]\n",
      "Q-value for action (2, 0, 1, 0): [2.6247102  1.54804802 2.22462123 1.59994784 1.59994784 1.79727389]\n",
      "Q-value for action (1, 2, 2, 0): [1.56380538 1.27209434 1.88472731 0.85664835 0.85664835 0.76912636]\n",
      "Q-value for action (0, 2, 2, 0): [3.11216933 1.68498238 1.85384448 3.3363122  3.3363122  2.06645322]\n",
      "Q-value for action (2, 2, 2, 0): [2.6247102  1.54804802 2.22462123 1.59994784 1.59994784 1.79727389]\n",
      "Q-value for action (3, 2, 2, 0): [7.37532757 2.01794195 5.17899194 6.98334147 6.98334147 3.26578654]\n",
      "Q-value for action (3, 4, 3, 0): [7.37532757 2.01794195 5.17899194 6.98334147 6.98334147 3.26578654]\n",
      "Q-value for action (0, 4, 3, 0): [3.11216933 1.68498238 1.85384448 3.3363122  3.3363122  2.06645322]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 5.17899194 6.98334147 6.98334147 3.26578654]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 2.22462123 1.59994784 1.59994784 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 5.17899194 6.98334147 6.98334147 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 1.85384448 3.3363122  3.3363122  2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 5.17899194 6.98334147 6.98334147 3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 2.22462123 1.59994784 1.59994784 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 2, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   0   3   3   3 |\n",
      "|  0   0   2   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(0, 0, 3, 1), (1, 0, 5, 1), (1, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Q-value for action (0, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Up (Raw action: (0, 0, 3, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   3   0   3   3 |\n",
      "|  0   0   2   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (2, 0, 1, 0), (1, 1, 2, 0), (0, 1, 2, 0), (3, 1, 2, 0), (2, 1, 2, 0), (3, 4, 3, 0), (0, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 0.         1.88472731 0.85664835 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.         1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (2, 0, 1, 0): [2.6247102  1.54804802 0.         2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (1, 1, 2, 0): [1.56380538 1.27209434 0.         1.88472731 0.85664835 0.76912636]\n",
      "Q-value for action (0, 1, 2, 0): [3.11216933 1.68498238 0.         1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (3, 1, 2, 0): [ 7.37532757  2.01794195 -3.26861583  5.17899194  6.98334147  3.26578654]\n",
      "Q-value for action (2, 1, 2, 0): [2.6247102  1.54804802 0.         2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (3, 4, 3, 0): [ 7.37532757  2.01794195 -3.26861583  5.17899194  6.98334147  3.26578654]\n",
      "Q-value for action (0, 4, 3, 0): [3.11216933 1.68498238 0.         1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (3, 4, 4, 0): [ 7.37532757  2.01794195 -3.26861583  5.17899194  6.98334147  3.26578654]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 0.         2.22462123 1.59994784 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [ 7.37532757  2.01794195 -3.26861583  5.17899194  6.98334147  3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.         1.85384448 3.3363122  2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [ 7.37532757  2.01794195 -3.26861583  5.17899194  6.98334147  3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 0.         2.22462123 1.59994784 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 1, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   0   3   3 |\n",
      "|  0   0   0   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(0, 0, 4, 1), (1, 0, 5, 1), (3, 1, 3, 1), (1, 1, 3, 1), (0, 1, 3, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Q-value for action (0, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Up (Raw action: (0, 0, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   3   0   3 |\n",
      "|  0   0   0   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   1   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (2, 0, 2, 0), (3, 4, 3, 0), (0, 4, 3, 0), (3, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.52103966 0.85664835 1.88472731 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (1, 0, 2, 0): [1.56380538 1.27209434 1.52103966 0.85664835 1.88472731 0.76912636]\n",
      "Q-value for action (2, 0, 2, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.22462123 1.79727389]\n",
      "Q-value for action (3, 4, 3, 0): [7.37532757 2.01794195 3.58134333 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (0, 4, 3, 0): [3.11216933 1.68498238 0.66630853 3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 3.58134333 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.22462123 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 3.58134333 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 3.3363122  1.85384448 2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 3.58134333 6.98334147 5.17899194 3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.22462123 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 4, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   3   0   3 |\n",
      "|  0   0   0   4   4   0 |\n",
      "|  3   0   0   0   0   0 |\n",
      "|  0   0   0   1   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(2, 0, 3, 1), (1, 0, 5, 1), (0, 0, 5, 1), (1, 1, 3, 1), (0, 1, 3, 1), (3, 1, 4, 1), (1, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1)]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 1, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   3   0   3 |\n",
      "|  0   0   0   4   0   0 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   1   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (2, 0, 2, 0), (3, 3, 3, 0), (1, 3, 3, 0), (0, 3, 3, 0), (2, 3, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.52103966 0.85664835 1.56380538 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 3.3363122  3.11216933 2.06645322]\n",
      "Q-value for action (1, 0, 2, 0): [1.56380538 1.27209434 1.52103966 0.85664835 1.56380538 0.76912636]\n",
      "Q-value for action (2, 0, 2, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.6247102  1.79727389]\n",
      "Q-value for action (3, 3, 3, 0): [7.37532757 2.01794195 3.58134333 6.98334147 7.37532757 3.26578654]\n",
      "Q-value for action (1, 3, 3, 0): [1.56380538 1.27209434 1.52103966 0.85664835 1.56380538 0.76912636]\n",
      "Q-value for action (0, 3, 3, 0): [3.11216933 1.68498238 0.66630853 3.3363122  3.11216933 2.06645322]\n",
      "Q-value for action (2, 3, 3, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.6247102  1.79727389]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 3.58134333 6.98334147 7.37532757 3.26578654]\n",
      "Q-value for action (0, 4, 4, 0): [3.11216933 1.68498238 0.66630853 3.3363122  3.11216933 2.06645322]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.6247102  1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 3.58134333 6.98334147 7.37532757 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 3.3363122  3.11216933 2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 3.58134333 6.98334147 7.37532757 3.26578654]\n",
      "Q-value for action (3, 5, 3, 0): [7.37532757 2.01794195 3.58134333 6.98334147 7.37532757 3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.52970111 1.59994784 2.6247102  1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 3, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   3   0   3 |\n",
      "|  0   0   0   4   0   0 |\n",
      "|  3   0   0   1   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(2, 0, 3, 1), (1, 0, 5, 1), (0, 0, 5, 1), (0, 1, 3, 1), (2, 1, 3, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (3, 2, 4, 1), (1, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (2, 0, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 3, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Down (Raw action: (2, 0, 3, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   0   3   3 |\n",
      "|  0   0   0   4   0   0 |\n",
      "|  3   0   0   1   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (2, 0, 2, 0), (1, 2, 3, 0), (0, 2, 3, 0), (3, 2, 3, 0), (2, 2, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.52103966 1.88472731 0.76912636 0.76912636]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 1.85384448 2.06645322 2.06645322]\n",
      "Q-value for action (1, 0, 2, 0): [1.56380538 1.27209434 1.52103966 1.88472731 0.76912636 0.76912636]\n",
      "Q-value for action (2, 0, 2, 0): [2.6247102  1.54804802 1.52970111 2.22462123 1.79727389 1.79727389]\n",
      "Q-value for action (1, 2, 3, 0): [1.56380538 1.27209434 1.52103966 1.88472731 0.76912636 0.76912636]\n",
      "Q-value for action (0, 2, 3, 0): [3.11216933 1.68498238 0.66630853 1.85384448 2.06645322 2.06645322]\n",
      "Q-value for action (3, 2, 3, 0): [7.37532757 2.01794195 3.58134333 5.17899194 3.26578654 3.26578654]\n",
      "Q-value for action (2, 2, 3, 0): [2.6247102  1.54804802 1.52970111 2.22462123 1.79727389 1.79727389]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 3.58134333 5.17899194 3.26578654 3.26578654]\n",
      "Q-value for action (0, 4, 4, 0): [3.11216933 1.68498238 0.66630853 1.85384448 2.06645322 2.06645322]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.52970111 2.22462123 1.79727389 1.79727389]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 3.58134333 5.17899194 3.26578654 3.26578654]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 1.85384448 2.06645322 2.06645322]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 3.58134333 5.17899194 3.26578654 3.26578654]\n",
      "Q-value for action (3, 5, 3, 0): [7.37532757 2.01794195 3.58134333 5.17899194 3.26578654 3.26578654]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.52970111 2.22462123 1.79727389 1.79727389]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 2, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   0   3   3 |\n",
      "|  0   0   0   2   0   0 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(1, 0, 4, 1), (0, 0, 4, 1), (1, 0, 5, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (3, 2, 4, 1), (1, 2, 4, 1), (0, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (1, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 0, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   0   0   3 |\n",
      "|  0   0   0   2   3   0 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (2, 0, 2, 0), (3, 1, 3, 0), (1, 1, 3, 0), (0, 1, 3, 0), (2, 1, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Chosen action for player 0: Move Right (Raw action: (1, 0, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   0   0   0   3 |\n",
      "|  0   0   1   2   3   0 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(1, 0, 5, 1), (0, 0, 5, 1), (3, 1, 4, 1), (2, 1, 4, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (1, 2, 4, 1), (0, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (1, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 0, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 0, 5, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   0   0   0   0 |\n",
      "|  0   0   1   2   3   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (2, 0, 1, 0), (3, 1, 2, 0), (1, 1, 2, 0), (0, 1, 2, 0), (3, 1, 3, 0), (1, 1, 3, 0), (2, 1, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [ 1.56380538  1.27209434  1.61388185 -0.0278649   1.73349974  1.73349974]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 3.5238609  1.34508139 1.8395128  1.8395128 ]\n",
      "Q-value for action (2, 0, 1, 0): [ 2.6247102   1.54804802  1.5197529  -0.0823117   1.85617231  1.85617231]\n",
      "Q-value for action (3, 1, 2, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (1, 1, 2, 0): [ 1.56380538  1.27209434  1.61388185 -0.0278649   1.73349974  1.73349974]\n",
      "Q-value for action (0, 1, 2, 0): [3.11216933 1.68498238 3.5238609  1.34508139 1.8395128  1.8395128 ]\n",
      "Q-value for action (3, 1, 3, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (1, 1, 3, 0): [ 1.56380538  1.27209434  1.61388185 -0.0278649   1.73349974  1.73349974]\n",
      "Q-value for action (2, 1, 3, 0): [ 2.6247102   1.54804802  1.5197529  -0.0823117   1.85617231  1.85617231]\n",
      "Q-value for action (3, 4, 4, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (0, 4, 4, 0): [3.11216933 1.68498238 3.5238609  1.34508139 1.8395128  1.8395128 ]\n",
      "Q-value for action (2, 4, 4, 0): [ 2.6247102   1.54804802  1.5197529  -0.0823117   1.85617231  1.85617231]\n",
      "Q-value for action (3, 5, 1, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 3.5238609  1.34508139 1.8395128  1.8395128 ]\n",
      "Q-value for action (3, 5, 2, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (3, 5, 3, 0): [ 7.37532757  2.01794195  6.89578093 -3.68730335  3.51996765  3.51996765]\n",
      "Q-value for action (2, 5, 4, 0): [ 2.6247102   1.54804802  1.5197529  -0.0823117   1.85617231  1.85617231]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 1, 2, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   0   0   0 |\n",
      "|  0   0   0   2   3   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(3, 1, 4, 1), (3, 1, 5, 1), (1, 1, 5, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (1, 2, 4, 1), (0, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (3, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Left (Raw action: (3, 1, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   0   3   0 |\n",
      "|  0   0   0   2   0   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (2, 0, 2, 0), (3, 1, 3, 0), (1, 1, 3, 0), (0, 1, 3, 0), (2, 1, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [ 1.56380538  1.27209434  1.52103966 -0.0278649   0.76912636  1.73349974]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 1.34508139 2.06645322 1.8395128 ]\n",
      "Q-value for action (1, 0, 2, 0): [ 1.56380538  1.27209434  1.52103966 -0.0278649   0.76912636  1.73349974]\n",
      "Q-value for action (2, 0, 2, 0): [ 2.6247102   1.54804802  1.52970111 -0.0823117   1.79727389  1.85617231]\n",
      "Q-value for action (3, 1, 3, 0): [ 7.37532757  2.01794195  3.58134333 -3.68730335  3.26578654  3.51996765]\n",
      "Q-value for action (1, 1, 3, 0): [ 1.56380538  1.27209434  1.52103966 -0.0278649   0.76912636  1.73349974]\n",
      "Q-value for action (0, 1, 3, 0): [3.11216933 1.68498238 0.66630853 1.34508139 2.06645322 1.8395128 ]\n",
      "Q-value for action (2, 1, 3, 0): [ 2.6247102   1.54804802  1.52970111 -0.0823117   1.79727389  1.85617231]\n",
      "Q-value for action (3, 4, 4, 0): [ 7.37532757  2.01794195  3.58134333 -3.68730335  3.26578654  3.51996765]\n",
      "Q-value for action (0, 4, 4, 0): [3.11216933 1.68498238 0.66630853 1.34508139 2.06645322 1.8395128 ]\n",
      "Q-value for action (2, 4, 4, 0): [ 2.6247102   1.54804802  1.52970111 -0.0823117   1.79727389  1.85617231]\n",
      "Q-value for action (3, 5, 1, 0): [ 7.37532757  2.01794195  3.58134333 -3.68730335  3.26578654  3.51996765]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 1.34508139 2.06645322 1.8395128 ]\n",
      "Q-value for action (3, 5, 2, 0): [ 7.37532757  2.01794195  3.58134333 -3.68730335  3.26578654  3.51996765]\n",
      "Q-value for action (3, 5, 3, 0): [ 7.37532757  2.01794195  3.58134333 -3.68730335  3.26578654  3.51996765]\n",
      "Q-value for action (2, 5, 4, 0): [ 2.6247102   1.54804802  1.52970111 -0.0823117   1.79727389  1.85617231]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 1, 3, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   2   3   0 |\n",
      "|  0   0   0   0   0   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(1, 0, 4, 1), (2, 0, 4, 1), (3, 1, 5, 1), (1, 1, 5, 1), (0, 1, 5, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (3, 2, 4, 1), (1, 2, 4, 1), (0, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (1, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 0, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Right (Raw action: (1, 0, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   2   0   0 |\n",
      "|  0   0   0   0   3   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 3, 0), (3, 4, 4, 0), (0, 4, 4, 0), (2, 4, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.52103966 1.27209434 1.73349974 1.73349974]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 1.68498238 1.8395128  1.8395128 ]\n",
      "Q-value for action (1, 0, 2, 0): [1.56380538 1.27209434 1.52103966 1.27209434 1.73349974 1.73349974]\n",
      "Q-value for action (1, 0, 3, 0): [1.56380538 1.27209434 1.52103966 1.27209434 1.73349974 1.73349974]\n",
      "Q-value for action (2, 0, 3, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.85617231 1.85617231]\n",
      "Q-value for action (3, 4, 4, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.51996765 3.51996765]\n",
      "Q-value for action (0, 4, 4, 0): [3.11216933 1.68498238 0.66630853 1.68498238 1.8395128  1.8395128 ]\n",
      "Q-value for action (2, 4, 4, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.85617231 1.85617231]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.51996765 3.51996765]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 1.68498238 1.8395128  1.8395128 ]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.51996765 3.51996765]\n",
      "Q-value for action (3, 5, 3, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.51996765 3.51996765]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.85617231 1.85617231]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 4, 4, 0))\n",
      "\n",
      "Current state for player 1:\n",
      "|  0   2   1   2   0   0 |\n",
      "|  0   0   0   0   3   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 1: [(3, 1, 4, 1), (0, 1, 4, 1), (3, 1, 5, 1), (1, 1, 5, 1), (3, 2, 0, 1), (1, 2, 0, 1), (2, 2, 0, 1), (0, 2, 4, 1), (2, 2, 4, 1)]\n",
      "Q-value for action (3, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 1, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 1, 5, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (3, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (1, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 0, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (0, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Q-value for action (2, 2, 4, 1): [0. 0. 0. 0. 0. 0.]\n",
      "Chosen action for player 1: Move Left (Raw action: (3, 1, 4, 1))\n",
      "\n",
      "Current state for player 0:\n",
      "|  0   2   1   2   3   0 |\n",
      "|  0   0   0   0   0   3 |\n",
      "|  3   0   0   0   4   0 |\n",
      "|  0   0   0   0   1   0 |\n",
      "|  0   0   0   0   0   0 |\n",
      "|  0   2   2   2   2   0 |\n",
      "\n",
      "Legal actions for player 0: [(1, 0, 1, 0), (0, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 3, 0), (1, 3, 4, 0), (0, 3, 4, 0), (2, 3, 4, 0), (3, 3, 4, 0), (3, 5, 1, 0), (0, 5, 1, 0), (3, 5, 2, 0), (3, 5, 3, 0), (3, 5, 4, 0), (2, 5, 4, 0)]\n",
      "Q-value for action (1, 0, 1, 0): [1.56380538 1.27209434 1.52103966 1.27209434 0.76912636 1.73349974]\n",
      "Q-value for action (0, 0, 1, 0): [3.11216933 1.68498238 0.66630853 1.68498238 2.06645322 1.8395128 ]\n",
      "Q-value for action (1, 0, 2, 0): [1.56380538 1.27209434 1.52103966 1.27209434 0.76912636 1.73349974]\n",
      "Q-value for action (1, 0, 3, 0): [1.56380538 1.27209434 1.52103966 1.27209434 0.76912636 1.73349974]\n",
      "Q-value for action (2, 0, 3, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.79727389 1.85617231]\n",
      "Q-value for action (1, 3, 4, 0): [1.56380538 1.27209434 1.52103966 1.27209434 0.76912636 1.73349974]\n",
      "Q-value for action (0, 3, 4, 0): [3.11216933 1.68498238 0.66630853 1.68498238 2.06645322 1.8395128 ]\n",
      "Q-value for action (2, 3, 4, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.79727389 1.85617231]\n",
      "Q-value for action (3, 3, 4, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.26578654 3.51996765]\n",
      "Q-value for action (3, 5, 1, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.26578654 3.51996765]\n",
      "Q-value for action (0, 5, 1, 0): [3.11216933 1.68498238 0.66630853 1.68498238 2.06645322 1.8395128 ]\n",
      "Q-value for action (3, 5, 2, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.26578654 3.51996765]\n",
      "Q-value for action (3, 5, 3, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.26578654 3.51996765]\n",
      "Q-value for action (3, 5, 4, 0): [7.37532757 2.01794195 3.58134333 2.01794195 3.26578654 3.51996765]\n",
      "Q-value for action (2, 5, 4, 0): [2.6247102  1.54804802 1.52970111 1.54804802 1.79727389 1.85617231]\n",
      "Chosen action for player 0: Move Left (Raw action: (3, 3, 4, 0))\n",
      "Game over.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Play the game\n",
    "agent.play()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
